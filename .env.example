# --- Development Settings ---
# Set to "true" to enable debug mode. In debug mode, Qdrant runs in-memory.
# In production (false), it connects to the specified QDRANTDB_URL.
DEBUG=true

# --- FastAPI Application Settings ---
# Name of the application, used in OpenAPI documentation.
APP_NAME="HCMUT Chatbot API"
# Version of the application.
APP_VERSION="0.1.0"
# Description of the application, used in OpenAPI documentation.
APP_DESCRIPTION="API for HCMUT Chatbot, providing query and document upload functionalities."
# Host address for the FastAPI application. "0.0.0.0" makes it accessible from other machines on the network.
APP_HOST="0.0.0.0"
# Port for the FastAPI application.
APP_PORT=8000
# Set to "true" to enable auto-reloading when code changes (for development).
APP_RELOAD=true
# CORS (Cross-Origin Resource Sharing) origins. "*" allows all origins.
# For production, specify your frontend URL, e.g., "http://localhost:3000,https://yourdomain.com"
APP_CORS_ORIGINS="*"
# Number of worker processes for Uvicorn.
APP_WORKERS=1

# --- Logging Settings ---
# Log level for the application. Options: TRACE, DEBUG, INFO, SUCCESS, WARNING, ERROR, CRITICAL
LOG_LEVEL="INFO"

# --- Qdrant Database Settings ---
# URL of your Qdrant instance. Required if DEBUG is false.
# Example: "http://localhost:6333" or your Qdrant Cloud URL.
QDRANTDB_URL="http://localhost:6333"
# API Key for your Qdrant instance. Required if DEBUG is false and your Qdrant instance is secured.
QDRANTDB_API_KEY="YOUR_QDRANT_API_KEY_IF_ANY"
# Batch size for writing documents to Qdrant.
DB_BATCH_SIZE=128
# Timeout in seconds for Qdrant operations.
DB_TIMEOUT=60

EMBEDDING_PROVIDER="huggingface" # "openai" or "huggingface" or "sentence_transformers"
EMBEDDING_HUGGINGFACE_API_KEY="YOUR_HUGGINGFACE_API_KEY_IF_USING_HF_INFERENCE_API"
# Base URL for your Hugging Face Text Embeddings Inference (TEI) server or HF Inference API.
# For TEI, e.g., "http://localhost:8080" (if TEI is running locally on port 8080).
EMBEDDING_HUGGINGFACE_BASE_URL="http://localhost:8080" # Example for local TEI
# "serverless_inference_api": For HF Serverless Inference API.
# "inference_endpoints": For HF Dedicated Inference Endpoints.
# "text_embeddings_inference": For self-hosted Text Embeddings Inference (TEI) server.
EMBEDDING_HUGGINGFACE_API_TYPE="text_embeddings_inference"

# --- OpenAI Embedding Settings (used if EMBEDDING_PROVIDER is "openai") ---
# API Key for OpenAI.
EMBEDDING_OPENAI_API_KEY="YOUR_OPENAI_API_KEY_IF_USING_OPENAI_EMBEDDINGS"
EMBEDDING_OPENAI_BASE_URL="https://api.openai.com/v1"

# --- General Embedding Configuration ---
EMBEDDING_MODEL="bkai-foundation-models/vietnamese-bi-encoder"
EMBEDDING_DIM=768
EMBEDDING_TOP_K=5
EMBEDDING_THRESHOLD=0.70

# --- Large Language Model (LLM) Settings (Currently configured for OpenAI) ---
LLM_OPENAI_API_KEY="YOUR_OPENAI_API_KEY_FOR_LLM"
LLM_OPENAI_BASE_URL="https://api.openai.com/v1"
LLM_OPENAI_MODEL="gpt-3.5-turbo"
LLM_TEMPERATURE=0.7
LLM_MAX_TOKENS=1000

# --- Cache Settings ---
CACHE_ENABLED="false"
FAQ_ENABLE_PARAPHRASING="true"